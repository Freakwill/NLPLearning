{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning NLP with nltk\n",
    "\n",
    "## Statistics\n",
    "\n",
    "bigrams/trigrams\n",
    "\n",
    "FreqDist/ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'one', 'dimension,', 'for', '$1', '<']\n",
      "['in', '$L^p$~norm', 'to', '$f$;', 'however,', 'the']\n",
      "['in', 'applications', 'almost', 'always', 'rely', 'on']\n",
      "['in', '$P_J', 'f$', 'converges', 'absolutely', 'since']\n",
      "['in', 'the', 'next', 'section', '(though', 'for']\n",
      "['in', '[3].', 'The', 'non-linear', 'hard', 'and']\n",
      "['in', 'applications', '(see', '[1],[2])', 'but', 'this']\n",
      "['in', 'an', 'arbitrary', 'order,', 'and', '(with']\n",
      "['in', 'this', 'paper', 'originated', 'from', 'a']\n",
      "['in', 'magnitude', 'by', 'the', 'maximal', 'function.']\n",
      "['in', '[5].', 'Let', '$f$', 'be', 'an']\n",
      "['in', '$L^p(\\\\R)$.', 'We', 'first', 'show', 'that']\n",
      "['in', 'an', '$L^p$', 'class', 'for', 'some']\n",
      "['in', '$C^\\\\infty_0(\\\\R)$', 'for', 'which', '$|M(f-g)(x)|', '\\\\leq']\n",
      "['in', 'the', 'expansion', 'of', '$h$,', 'and']\n",
      "['in', '[3]', 'under', 'weaker', 'decay', 'hypotheses']\n",
      "['in', 'a', 'distributional', 'sense', 'to', '$\\\\delta(x-y)$.']\n",
      "['in', 'an', '$L^p$', 'class', 'for', 'some']\n",
      "['in', 'the', 'summation', 'for', '$T_\\\\lambda', 'f(x)$']\n",
      "['in', 'the', 'above', 'argument;', 'we', 'omit']\n",
      "['in', '[3].', '{\\\\heading', '2.', 'Proof', 'of']\n",
      "['in', 'the', 'proof', 'of', 'Theorem', '1,']\n",
      "['in', 'the', 'previous', 'section', 'to', 'bound']\n",
      "['in', '[3].', 'Now', 'suppose', 'that', '$\\\\alpha']\n",
      "['in', '$L^p([0,1))$', 'for', 'every', '$1', '<']\n",
      "['in', '(b).', 'We', 'shall', 'define', '$f_N$']\n",
      "['in', 'the', 'sum', 'defining', '$f_N(x)$', 'are']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def twoword(s):\n",
    "    # delete the nonwords\n",
    "    # get bigrams, combine them with ' '\n",
    "    return (a+' '+b for a, b in nltk.bigrams(w for w in s.split() if w.isalpha()))\n",
    "\n",
    "def fromFile(file, hook=twoword):\n",
    "    with open(file) as fo:\n",
    "        return nltk.FreqDist(hook(fo.read()))\n",
    "\n",
    "corpus_root = '/Users/william/Folders/mycorpus/'\n",
    "\n",
    "def follow(s, w, n=1):\n",
    "    # print w and words following it in s\n",
    "    # example: ['in', 'one', 'dimension,', 'for', '$1', '<']\n",
    "    k = 0\n",
    "    while k < len(s):\n",
    "        a = s[k]\n",
    "        if a == w:\n",
    "            print(s[k:k+n+1])\n",
    "            k += n\n",
    "        else:\n",
    "            k += 1\n",
    "            \n",
    "f = corpus_root + 'wavelets.tex'\n",
    "with open(f) as fo:\n",
    "    s = fo.read()\n",
    "s = s.split()\n",
    "follow(s, 'in', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read texts from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', '2003,', 'Grisha', 'Perelman', 'signiﬁcantly', 'advanced']\n",
      "['in', 'a', 'program', 'to', 'understand', 'the']\n",
      "['in', '[29]', 'Perelman', 'showed', 'that', 'in']\n",
      "['in', 'three', 'spatial', 'dimensions', 'there', 'was']\n",
      "['in', 'ﬁnite', 'time', '(thus', 'there', 'exists']\n",
      "['in', 'particular', 'includes', 'the', 'projective', 'space']\n",
      "['in', '[28],', '[29],', '[30]', 'for', 'general']\n",
      "['in', 'fact', 'quite', 'natural,', 'because', 'the']\n",
      "['in', 'the', 'category', 'of', 'simply', 'connected']\n",
      "['in', 'communication', 'with', 'each', 'other', 'and']\n",
      "['in', 'every', 'important', 'detail.', 'In', 'particular']\n",
      "['in', 'fact', 'will', 'commit', 'a', 'number']\n",
      "['in', 'order', 'to', 'suppress', 'certain', 'technical']\n",
      "['in', 'the', 'proof', 'of', 'the', 'true']\n",
      "['in', 'an', 'ahistorical', 'order,', 'in', 'order']\n",
      "['in', 'several', 'other', 'places', '(see', 'e.g.']\n",
      "['in', 'the', 'ﬁeld', 'have', 'also', 'made']\n",
      "['in', '[5],', '[22],', '[27]', 'to', 'fully']\n",
      "['in', '[5],', '[22],', '[27]', 'for', 'a']\n",
      "['in', 'other', 'papers', 'when', 'appropriate.', '10In']\n",
      "['in', 'Section', '4.', 'With', 'the', 'local']\n",
      "['in', 'hand,', 'Theorem', '1.2', 'then', 'reduces']\n",
      "['in', 'special', 'cases', 'by', 'Hamilton', '[20]']\n",
      "['in', 'general.', 'One', 'ma', 'jor', 'obstacle']\n",
      "['in', 'nature,', 'which', 'meant', 'that', 'they']\n",
      "['in', 'Perelman’s', 'work', 'is', 'to', 'introduce']\n",
      "['in', 'turn', 'on', 'the', 'Cheeger-Gromov', 'theory']\n",
      "['in', 'ﬁnite', 'time', 'with', 'no', 'intervening']\n",
      "['in', 'the', 'hy-', 'potheses', 'and', 'conclusions,']\n",
      "['in', 'detail', 'in', '[27,', '§18].', 'While']\n",
      "['in', 'nature', 'than', 'those', 'in', '[28],']\n",
      "['in', 'which', 'one', 'returns', 'back', 'from']\n",
      "['in', 'the', 'references,', 'but', 'the', 'author']\n",
      "['in', '[22],', '[27],', '[5],', 'in', 'addition']\n",
      "['in', 'parallel', '(in', 'particular,', 'switching', 'from']\n",
      "['in', 'order', 'to', 'slightly', 'simplify', 'the']\n",
      "['in', 'coordinates', 'via', 'the', 'Christoﬀel', 'symbol']\n",
      "['in', 'turn', 'leads', 'to', 'the', 'Riemann']\n",
      "['in', 'turn', 'contracts', 'to', 'form', 'the']\n",
      "['in', 'mind', 'the', 'schematic', 'form', 'of']\n",
      "['in', 'the', 'formula', 'for', 'the', 'conformal']\n",
      "['in', 'every', 'expression.', 'Ricci', 'ﬂow', 'will']\n",
      "['in', 'Perelman’s', 'work17.', 'The', 'Ricci', 'ﬂow,']\n",
      "['in', 'some', 'time', 'interval.', 'It', 'is']\n",
      "['in', 'order', 'for', 'Ricci', 'ﬂow', 'to']\n",
      "['in', 'the', 'forward', 'time', 'direction.', 'Intuitively,']\n",
      "['in', 'which', 'the', 'thre', 'PERELMAN’S', 'PROOF']\n",
      "['in', 'the', 'presence', 'of', 'a', 'critical']\n",
      "['in', 'the', 'pres-', 'ence18', 'of', 'a']\n",
      "['in', 'coordinates', 'by', 't', 'gαβ', '(t,']\n",
      "['in', 'the', 'form', '(1)', '(2)', ')']\n",
      "['in', 'the', 'case', 'of', 'indeﬁnite', 'curvature.']\n",
      "['in', 'that', 'bounds', 'on', 'such', 'quantities']\n",
      "['in', 'these', 'notes', 'in', 'terms', 'of']\n",
      "['in', 'this', 'paper,', 'as', 'powers', 'of']\n",
      "['in', 'coordinates', 'we', 'obtain', '−2Ricαβ', '=']\n",
      "['in', 'the', 'equation.', 'The', 'second', 'approach,']\n",
      "['in', 'other', 'words', '∂t', 'g', '=']\n",
      "['in', 'the', 'local', 'theory,', 'is', 'not']\n",
      "['in', 'the', 'global', 'theory;', 'it', 'is']\n",
      "['in', 'its', 'original', 'form', 'seems', 'well']\n",
      "['in', 'time', 'for', 'all', 'time', 't']\n",
      "['in', 'the', 'analysis', 'of', 'blowup', 'singularities,']\n",
      "['in', 'other', 'equations', '(particularly', 'equations', 'with']\n",
      "['in', 'time', 'up', 'to', 'diﬀeomorphism.', 'More']\n",
      "['in', 'the', 'orthonormal', 'frame', 'coordinates24.', 'These']\n",
      "['in', 'ﬁnite', 'time,', 'the', 'metric', 'also']\n",
      "['in', 'the', 'positive', 'Ricci', 'curvature', 'case.']\n",
      "['in', 'magnitude,', 'the', 'negative', 'portion', 'of']\n",
      "['in', 'fact', 'controls', 'in', 'magnitude', 'all']\n",
      "['in', '[21].', 'Roughly', 'speaking,', 'it', 'is']\n",
      "['in', '(9),', 'the', 'set', 'of', 'points']\n",
      "['in', 'this', 'setting.', 'PERELMAN’S', 'PROOF', 'OF']\n",
      "['in', 'high', 'curvature', 'regions,', 'almost', 'all']\n",
      "['in', 'the', 'classiﬁcation', 'of', 'various', 'types']\n",
      "['in', 'the', 'argument;', 'see', '[27,', '§4.4],']\n",
      "['in', 'time,', 'with', 'the', 'rate', 'determined']\n",
      "['in', 'order', 'to', 'show', 'that', 'the']\n",
      "['in', 'the', 'next', 'section.', 'Furthermore,', 'there']\n",
      "['in', 'performing', 'such', 'extensions,', 'but', 'see']\n",
      "['in', 'the', 'manifold,', 'but', 'from', 'embedded']\n",
      "['in', 'the', 'non-negatively', 'curved', 'case,', 'namely']\n",
      "['in', 'geometry;', 'to', 'pick', 'just', 'one']\n",
      "['in', 'Gromov’s', 'ground-breaking', 'paper', '[15]', 'on']\n",
      "['in', 'a', 'suitably', 'weak', 'sense)', 'whenever']\n",
      "['in', 'ﬁnite', 'time,', 'which', 'is', 'absurd.']\n",
      "['in', 'ﬁnite', 'time', 'whenever', 'π2', '(M']\n",
      "['in', 'the', 'simply', 'connected', 'case.', 'If']\n",
      "['in', 'the', 'literature.', 'There', 'is', 'an']\n",
      "['in', 'M', 'as', 'loops', 'of', 'two-spheres,']\n",
      "['in', 'this', 'way', 'along', 'a', 'two-sphere']\n",
      "['in', 'the', 'simply', 'connected', 'case).', 'PERELMAN’S']\n",
      "['in', 'a', 'controllable', 'manner.', 'These', '(highly']\n",
      "['in', 'the', 'sub', 'ject', 'was', 'to']\n",
      "['in', '[28,', '§3,7]', 'several', 'useful', 'critical']\n",
      "['in', 'spacetime', 'of', 'a', 'solution', 'M']\n",
      "['in', 'd', 'spatial', 'dimensions,', 'thus', 'x0']\n",
      "['in', 'time;', 'this', 'is', 'basically', 'measuring']\n",
      "['in', 'a', 'diﬀerent', 'context', 'than', 'Ricci']\n",
      "['in', 'spirit', 'to', 'the', 'Ricci', 'curvature']\n",
      "['in', 'other', 'geometric', 'formulae', 'such', 'as']\n",
      "['in', 'the', 'conformal', 'Laplacian.', 'See', '[28,']\n",
      "['in', 'time,', 'use', 'variation', 'formulae', 'to']\n",
      "['in', 'the', 'absence', 'of', 'geometric', 'intuition.']\n",
      "['in', 'fact', 'constant', 'in', 'the', 'case']\n",
      "['in', 'terms', 'of', 'inﬁnite', 'dimensional', 'Rie-']\n",
      "['in', '[11].', 'Comparing', 'the', 'reduced', 'volume']\n",
      "['in', 'the', 'case', 'where', 'the', 'ball']\n",
      "['in', 'the', 'sense', 'that', '|Riem|g', '=']\n",
      "['in', 'turn', 'leads', 'to', 'an', 'estimate']\n",
      "['in', '(5)', '(if', 'we', 'normalise', 't0']\n",
      "['in', 'the', 'argument.', 'Details', 'can', 'be']\n",
      "['in', '[28,', '§7],', '[27,', '§9.2],', '[22,']\n",
      "['in', 'the', 'monotonicity', 'formula', 'as', 'in']\n",
      "['in', 'the', 'previous', 'paragraph,', 'one', 'can']\n",
      "['in', 'fact', 'conclude', 'that', 'the', 'reduced']\n",
      "['in', 'several', 'stages.', 'The', 'ﬁrst', 'ma']\n",
      "['in', 'PERELMAN’S', 'PROOF', 'OF', 'POINCAR', '´E']\n",
      "['in', 'which', 'the', 'curvature', 'is', 'bounded']\n",
      "['in', 'the', 'non-collapsed', 'case,', 'a', 'famous']\n",
      "['in', 'constraining', 'the', 'behaviour', 'of', 'manifolds.']\n",
      "['in', 'Perelman’s', 'work,', 'and', 'also', 'in']\n",
      "['in', 'subsequent', 'papers.', 'In', 'particular,', 'in']\n",
      "['in', 'order', 'to', 'fully', 'complete', 'the']\n",
      "['in', 'a', 'scale-invariant', 'fashion.', '9.', 'Blowup']\n",
      "['in', 'principle)', 'to', 'analyse', 'the', 'asymptotic']\n",
      "['in', 'the', 'analysis', 'of', 'any', 'nonlinear']\n",
      "['in', 'particular', 'is', 'essential', 'for', 'deﬁning']\n",
      "['in', 'a', 'best-case', 'scenario.', 'Suppose', 'we']\n",
      "['in', 'fact', 'if', 'the', 'xn', 'are']\n",
      "['in', 'order', 'to', 'reduce', 'their', 'curvature']\n",
      "['in', 'topology', '(and', 'not', 'disrupting', 'the']\n",
      "['in', 'the', 'classiﬁcation', 'of', 'curvature', 'regions,']\n",
      "['in', 'that', 'the', 'ancient', 'solutions', 'themselves']\n",
      "['in', 'other', 'words,', 'one', 'needs', 'to']\n",
      "['in', 'turn', 'will', 'lead', 'to', 'a']\n",
      "['in', 'mind:', '•', '(The', 'sphere)', 'For']\n",
      "['in', 'earlier', 'sections,', 'while', 'the', 'latter']\n",
      "['in', 'two', 'spatial', 'dimensions,', 'the', 'cylinder']\n",
      "['in', 'this', 'case.', '(Also,', 'we', 'require']\n",
      "['in', 'two', 'spatial', 'dimensions', 'instead', 'of']\n",
      "['in', 'three', 'dimensions,', 'see', 'e.g.', '[2],']\n",
      "['in', 'Section', '14.', '•', '(A', 'non-example)']\n",
      "['in', 'fact', 'al', 'l', 'κ-solutions', 'essentially']\n",
      "['in', 'a', 'certain', 'quantitative', 'sense,', 'after']\n",
      "['in', 'several', 'stages;', 'we', 'now', 'consider']\n",
      "['in', 'turn.', '10.1.', 'Bounded', 'curvature.', 'We']\n",
      "['in', '[27,', '§9.3],', '[22,', '§45],', '[5,']\n",
      "['in', 'two', 'dimensions).', 'The', 'argument', 'runs']\n",
      "['in', 'fact', 'furthermore', 'that', 'the', 'manifold']\n",
      "['in', 'fact', 'have', 'constant', 'curvature,', 'so']\n",
      "['in', 'the', 'asymptotic', 'regime', 'x', '→']\n",
      "['in', 'which', 'supd(x,xk', ')∼rk', 'R(t,', 'x)']\n",
      "['in', 'particular,', 'as', 'κ-solutions', 'are', 'non-collapsed,']\n",
      "['in', 'fact', 'that', 'the', 'limit', 'is']\n",
      "['in', 'a', 'volume', 'sense;', 'using', 'Bishop-Gromov']\n",
      "['in', '[28],', '[22],', '[5]),', 'or', 'by']\n",
      "['in', '[27]).', '51This', 'is', 'consistent', 'with']\n",
      "['in', 'order', 'to', 'lower', 'bound', 'the']\n",
      "['in', 'Fatou’s', 'lemma', 'need', 'not', 'be']\n",
      "['in', 'a', 'distributional', 'sense;', 'see', '[5,']\n",
      "['in', 'the', 'proof.', 'If', 'the', 'soliton']\n",
      "['in', '(5),', 'one', 'can', 'show', 'that']\n",
      "['in', 'the', 'preceding', 'paragraph', 'then', 'let']\n",
      "['in', 'a', 'Ricci', 'ﬂow', 'lives', 'in']\n",
      "['in', 'a', 'rescaled', 'canonical', 'neighbourhood', 'if,']\n",
      "['in', 'the', 'center', 'of', 'a', 'region']\n",
      "['in', 'rescaled', 'time', 'for', 'a', '54The']\n",
      "['in', 'contrast,', 'are', 'a', 'gradient', 'steady']\n",
      "['in', 'a', 'compact', 'manifold', 'diﬀeomorphic', 'to']\n",
      "['in', 'a', 'open', 'manifold', 'which', 'is']\n",
      "['in', 'an', 'open', 'region', 'C', ',']\n",
      "['in', 'space', 'and', 'time).', 'Furthermore,', 'the']\n",
      "['in', 'a', 'κ-solution', 'lies', 'in', 'a']\n",
      "['in', 'one', 'direction,', 'then', 'application', 'of']\n",
      "['in', 'Section', '10.4', 'to', 'conclude', 'that']\n",
      "['in', 'a', 'non-compact', 'κ-', 'solution', 'either']\n",
      "['in', 'a', 'rescaled', 'ε-neck', 'or', 'is']\n",
      "['in', 'the', 'core', 'of', 'a', 'rescaled']\n",
      "['in', 'the', 'preceding', 'paragraph', 'shows', 'that']\n",
      "['in', 'M', 'is', 'contained', 'either', 'in']\n",
      "['in', 'the', 'rescaled', 'core', 'of', 'a']\n",
      "['in', 't).', 'So', 'we', 'may', 'assume']\n",
      "['in', '[28,', '§12]', 'had', 'some', 'inaccuracies,']\n",
      "['in', '[29,', '§6].', 'The', 'three', 'claims']\n",
      "['in', 'turn', 'lets59', 'us', 'use', 'Hamilton']\n",
      "['in', 'K', '(from', 'incredibly', 'huge', 'down']\n",
      "['in', 'rescaled', 'canonical', 'neighbourhoods,', 'and', 'want']\n",
      "['in', 'rescaled', 'neigh-', 'bourhoods', 'without', 'curvature']\n",
      "['in', 'rescaled', 'Ricci', 'ﬂows', 'where', 'the']\n",
      "['in', 'a', 'canonical', 'neighbourhood.', 'This', 'means']\n",
      "['in', 'each', 'Un', 'which', 'starts', 'at']\n",
      "['in', 'a', 'long', 'chain', 'of', 'rescaled']\n",
      "['in', 'contrast', 'to', 'an', 'ε-tube;', 'this']\n",
      "['in', 'the', 'next', 'section).', 'But', 'the']\n",
      "['in', 'space,', 'but', 'has', 'bounded', 'curvature']\n",
      "['in', 'Section', '10.4', 'to', 'deduce', '(iii)']\n",
      "['in', 'that', 'one', 'needs', 'to', 'extend']\n",
      "['in', 'time', 'somewhat', 'in', 'order', 'to']\n",
      "['in', 'Perelman’s', 'work,', 'but', 'see', '[22,']\n",
      "['in', 'Sections', '10.2,', '10.4).', 'At', 'this']\n",
      "['in', 'which', 'one', 'uses', 'parabolic', 'theory']\n",
      "['in', 'particular', 'the', 'scalar', 'curvature', 'stays']\n",
      "['in', '[28,', '§12],', '[29,', '§4],', '[27,']\n",
      "['in', 'a', 'Ricci', 'ﬂow', 'is', 'contained']\n",
      "['in', 'a', 'canonical', 'neighbourhood,', 'such', 'as']\n",
      "['in', 'these', 'regions', 'one', 'can', 'still']\n",
      "['in', 'canonical', 'neighbourhoods.', 'If', 'these', 'neighbourhoods']\n",
      "['in', 'addition', 'to', 'ε-round', 'and', 'C']\n",
      "['in', 'the', 'horn', 'of', 'suﬃciently', 'high']\n",
      "['in', 'a', 'δ', '-neck.', 'The', 'proof']\n",
      "['in', 'δ', '-necks.', 'Rescaling', 'and', 'taking']\n",
      "['in', 'an', 'ε-neck.', 'By', 'considering', 'what']\n",
      "['in', 'time', 'to', 'an', 'ancient', 'solution,']\n",
      "['in', 'fact', 'a', 'round', 'sphere,', 'and']\n",
      "['in', 'which', 'was', 'then', 'expanded', 'in']\n",
      "['in', '[5,', '§7.3],', '[27,', '§12],', 'or']\n",
      "['in', 'the', 'PERELMAN’S', 'PROOF', 'OF', 'POINCAR']\n",
      "['in', 'harmonic', 'analysis,', 'and', 'so', 'it']\n",
      "['in', 'R4', '(with', 'the', 'Riemannian', 'metric']\n",
      "['in', 'R3', 'in', 'the', 'usual', 'manner)']\n",
      "['in', 'particular', 'must', 'develop', 'a', 'singularity']\n",
      "['in', 'other', 'words,', 'T∗', '≤', '1.']\n",
      "['in', 'the', 'previous', 'section,', 'one', 'can']\n",
      "['in', '[5],', 'which', 'goes', 'through', 'the']\n",
      "['in', '[22,', '§59]', 'and', 'attributed', 'to']\n",
      "['in', 'Section', '11', 'would', 'imply', 'that']\n",
      "['in', 'Section', '11)', 'to', 'show', 'that']\n",
      "['in', 'a', 'ε-neck', 'or', '(C,', 'ε)-cap;']\n",
      "['in', 'general,', 'and', 'so', 'one', 'must']\n",
      "['in', 'the', 'interval', '[2n', ',', '2n+1']\n",
      "['in', 'Section', '12', 'we', 'know', 'that']\n",
      "['in', 'one', 'of', 'a', 'ﬁnite', 'number']\n",
      "['in', 'the', 'Ricci', 'ﬂow63', 'with', 'scalar']\n",
      "['in', 'a', 'horn', 'is', 'contained', 'in']\n",
      "['in', 'the', 'centre', 'of', 'a', 'δn']\n",
      "['in', 'the', 'non-rescaled', 'metric)', '-', 'this']\n",
      "['in', 'the', 'Ricci', 'ﬂow', 'with', 'surgery,']\n",
      "['in', 'these', 'components', 'is', 'contained', 'in']\n",
      "['in', 'a', 'canonical', 'neighbourhood.', 'Some', 'topology-chasing']\n",
      "['in', 'order', 'to', 'focus', 'on', 'the']\n",
      "['in', 'time', 'to', 'desingularise', 'the', 'pre-surgery']\n",
      "['in', 'ﬁnite', 'time.', '(ii)', 'The', 'surgery']\n",
      "['in', 'the', 'above', 'analy-', 'sis,', 'remains']\n",
      "['in', 'the', 'presence', 'of', 'surgery.', 'In']\n",
      "['in', 'the', 'limit', 't', '→', '∞.)']\n",
      "['in', 'a', 'single', 'dyadic', 'time', 'interval']\n",
      "['in', 'ﬁnite', 'time.', 'We', 'thank', 'John']\n",
      "['in', '[27,', '§18].', 'The', 'strategy', 'is']\n",
      "['in', 'Section', '6', 'to', 'Ricci', 'ﬂow']\n",
      "['in', 'the', 'direction', 'of', 'their', 'Ricci']\n",
      "['in', 'symplectic', 'manifolds,', 'Invent.', 'Math.', '82']\n",
      "['in', 'Ricci', 'ﬂow,', 'Surveys', 'in', 'Diﬀ.']\n",
      "['in', 'the', 'work', 'of', 'Perelman,', 'preprint.']\n",
      "['in', 'Mathematics', '171,', 'Springer-Verlag,', 'New', 'York,']\n",
      "['in', 'diﬀerential', 'geometry,', 'in', '‘Proceedings', 'of']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser,PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextBoxHorizontal,LAParams\n",
    "from pdfminer.pdfinterp import PDFTextExtractionNotAllowed\n",
    "\n",
    "\n",
    "def pdf2str(filename):\n",
    "    fp = open(filename, 'rb')\n",
    "    praser = PDFParser(fp)\n",
    "    doc = PDFDocument()\n",
    "    praser.set_document(doc)\n",
    "    doc.set_parser(praser)\n",
    "    doc.initialize()\n",
    "\n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        s = ''\n",
    "        for page in doc.get_pages():\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if isinstance(x, LTTextBoxHorizontal):\n",
    "                    results = x.get_text()\n",
    "                    s += results\n",
    "        return s\n",
    "\n",
    "s = pdf2str(corpus_root + 'terence.pdf')\n",
    "s = s.split()\n",
    "follow(s, 'in', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read texts from doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'分': 71, '，': 68, '学': 66, '\\n': 62, '工': 61, '1': 57, '2': 55, '。': 53, '的': 51, '江': 47, ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx\n",
    "#docx.Document(corpus_root+'zjc.doc')\n",
    "import codecs\n",
    "\n",
    "f = corpus_root+'zjjy.txt'\n",
    "with open(f, encoding='gb2312') as fo:\n",
    "    s = fo.read()\n",
    "# bg = nltk.bigrams([w for w in s.split() if w.isalpha()])\n",
    "# cfd = nltk.ConditionalFreqDist([(a, b) for a, b in bg])\n",
    "\n",
    "fd = nltk.FreqDist(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import PlaintextCorpusReader\n",
    "#wl = PlaintextCorpusReader(corpus_root, '.*')\n",
    "# wl.fileids()\n",
    "\n",
    "# Conditional Distribution\n",
    "import pathlib\n",
    "corpus_root = pathlib.Path('/Users/William/Folders/mycorpus')\n",
    "f = corpus_root / 'wavelets.tex'\n",
    "with open(f) as fo:\n",
    "    s = fo.read()\n",
    "bg = nltk.bigrams([w for w in s.split() if w.isalpha()])\n",
    "cfd = nltk.ConditionalFreqDist([(a, b) for a, b in bg])\n",
    "# cfd.tabulate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f0a5eeb9b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Lily loves flowers'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "raw = 'Lily loves flowers'\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "Important for linguistic analysis\n",
    "\n",
    "tokenize -> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning. Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "download corpus with `nltk.download('sinica_treebank')`\n",
    "\n",
    "Searched in:\n",
    "    - '/Users/william/nltk_data'\n",
    "    - '/Library/Frameworks/Python.framework/Versions/3.6/nltk_data'\n",
    "    - '/Library/Frameworks/Python.framework/Versions/3.6/share/nltk_data'\n",
    "    - '/Library/Frameworks/Python.framework/Versions/3.6/lib/nltk_data'\n",
    "    - '/usr/share/nltk_data'\n",
    "    - '/usr/local/share/nltk_data'\n",
    "    - '/usr/lib/nltk_data'\n",
    "    - '/usr/local/lib/nltk_data'\n",
    "    \n",
    "`nltk.app.concordance()`  concordance tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.tag.str2tuple('fly/NN') # => ('fly', 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()\n",
    "nltk.corpus.treebank.tagged_words()\n",
    "nltk.corpus.brown.tagged_words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of tag: word1, word2, ...\n",
    "memory = {}\n",
    "for word, tag in nltk.corpus.brown.tagged_words():\n",
    "    if tag in memory:\n",
    "        memory[tag].add(word)\n",
    "    else:\n",
    "        memory[tag] = {word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tags depend on their context in sentences\n",
    "tagged_sents = nltk.corpus.brown.tagged_sents(categories='news')\n",
    "tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-defined tagger\n",
    "\n",
    "DefaultTagger/RegexpTagger\n",
    "\n",
    "UnigramTagger/BigramTagger/...\n",
    "\n",
    "### Combine taggers\n",
    "```python\n",
    "t1= Tagger(model, backoff=t0)\n",
    "t2= Tagger(model, backoff=t1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4602999383415876"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.DefaultTagger('NN')\n",
    "# patterns = [(regexp, tag)]\n",
    "# nltk.RegexpTagger(patterns)\n",
    "\n",
    "# UnigramTagger({word:tag}, backoff)\n",
    "words = nltk.corpus.brown.words()\n",
    "tagged_words = nltk.corpus.brown.tagged_words()\n",
    "fd = nltk.FreqDist(words)\n",
    "cfd = nltk.ConditionalFreqDist(tagged_words)\n",
    "model = {word:cfd[word].max() for word in list(fd.keys())[:100]}\n",
    "tagger = nltk.UnigramTagger(model=model, backoff=nltk.DefaultTagger('NN'))\n",
    "tagger.evaluate(tagged_sents)\n",
    "\n",
    "tagger.tag(tagged_sents[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class MyTagger(nltk.TaggerI):\n",
    "    def __init__(self, train):\n",
    "        ...\n",
    "    def tag(self, sentence):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "```python\n",
    "train = [({'f':x}, y)]  # list of input-output pairs\n",
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "nltk.classify.accuracy(classifier, test)\n",
    "```\n",
    "\n",
    "[name_gender](https://github.com/Freakwill/name_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('names')\n",
    "\n",
    "# names = [(name, 'male') for name in nltk.corpus.names.words('male.txt')] + [(name, 'female') for name in nltk.corpus.names.words('female.txt')]\n",
    "# train, test = features[500:], featrues[:500]\n",
    "# classifier = nltk.NaiveBayesClassifier.train(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
    "# rtepair = nltk.corpus.rte.pairs('I am a Person', 'I love you')\n",
    "# extractor = nltk.classify.rte_classify.RTEFeatureExtractor(rtepair)\n",
    "# rte_features(rtepair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def entropy(xs):\n",
    "    fd = nltk.FreqDist(xs)\n",
    "    ps = [p for x, p in fd.items()]\n",
    "    # ps = list(fd.values())\n",
    "    return scipy.stats.entropy(ps, base=2)\n",
    "\n",
    "xs = ['male', 'female', 'male', 'male']\n",
    "entropy(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.entropy([2,2], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('the', 'DT'), ('cat', 'NN'), ('owned', 'VBN'), ('by', 'IN'), ('Lucy', 'NNP'), ('.', '.')]\n",
      "(S\n",
      "  (NP The/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN)\n",
      "  owned/VBN\n",
      "  by/IN\n",
      "  (NP Lucy/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence='The little yellow dog barked at the cat owned by Lucy.'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "\n",
    "# cp = nltk.RegexpParser(\"\")\n",
    "# result = cp.parse(tags)\n",
    "# print(result)\n",
    "\n",
    "grammar = r\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>}\n",
    "{<NNP>+}\n",
    "\"\"\"  # NP - chunking\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tags)  # grammar tree\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "NP\n",
      "NP\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "# subtrees of a grammar tree\n",
    "for subtree in result.subtrees():\n",
    "    print(subtree.label())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoNLL2000 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    ". . O\n",
    "\"\"\"\n",
    "nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('PP', [('Over', 'IN')]), Tree('NP', [('a', 'DT'), ('cup', 'NN')]), Tree('PP', [('of', 'IN')]), Tree('NP', [('coffee', 'NN')]), (',', ','), Tree('NP', [('Mr.', 'NNP'), ('Stone', 'NNP')]), Tree('VP', [('told', 'VBD')]), Tree('NP', [('his', 'PRP$'), ('story', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.corpus.conll2000.chunked_sents('train.txt')[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(S, (NP, VP))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(PP, (P, NP))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(NP, (Det, N))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(NP, (Det, N, PP))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(NP, ('I',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(VP, (V, NP))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(VP, (VP, PP))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(Det, ('an',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(Det, ('my',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(N, ('elephant',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(N, ('pajamas',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(V, ('shot',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(P, ('in',))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "for p in grammar.productions():\n",
    "    p.lhs(), p.rhs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(grammar)  # Chart Parser, WFST\n",
    "trees = parser.parse(sent)\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    \n",
    "# rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "# for i in rd_parser.parse(sent):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift-Reduce\n",
    "* Shift: 此解析器反复将下个输入词push进stack。\n",
    "* Reduce: if stack前n项，匹配表达式右侧的n个项目，then 弹出栈，and 将产生式左边项目压如栈。\n",
    "\n",
    "两个缺点:\n",
    "- 由于堆栈的特殊性，只能找到一种解析\n",
    "- 不能保证一定能找到解析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = nltk.ShiftReduceParser(grammar)\n",
    "list(parser.parse(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ProjectiveDependencyParser(grammar)\n",
    "trees = parser.parse(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Syntax\n",
    "\n",
    "## AVM\n",
    "\n",
    "$\\left[\\begin{align} POS & N &\\\\\n",
    " &  PER & 3 \\\\\n",
    "ARG    & NUM & pl \\\\\n",
    "    & GND & fem \n",
    "    \\end{align}\\right]$\n",
    "\n",
    "## Feature Struct\n",
    "\n",
    "```python\n",
    "fs0 = FeatStruct(PER=3, NUM='pl', GND='fem')\n",
    "fs = FeatStruct(POS='N', ARG=fs0)\n",
    "FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n] \n",
      "NP[NUM=?n] -> PropN[NUM=?n] \n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl] \n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children' \n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n",
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\n",
    "cp=nltk.load_parser('grammars/book_grammars/feat0.fcfg', trace=2)\n",
    "trees = cp.parse('Kim likes children'.split())\n",
    "\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic\n",
    "\n",
    "prover9\n",
    "\n",
    "[Howto](http://www.nltk.org/howto/logic.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem import Valuation, Model, Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq = Expression.fromstring('P -> Q')\n",
    "p = Expression.fromstring('P')\n",
    "q = Expression.fromstring('Q')\n",
    "prover = nltk.Prover9()\n",
    "prover.prove(q, [pq, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model semantics\n",
    "\n",
    "Model = Domain + Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = Valuation((('P', True), ('Q', True)))\n",
    "val['P']\n",
    "dom = set()\n",
    "g = nltk.Assignment(dom)\n",
    "m = nltk.Model(dom, val)\n",
    "m.evaluate('P & Q', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate Logic\n",
    "\n",
    "- expression\n",
    "- type\n",
    "- lambda expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LambdaExpression \\x.bullshit(x)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nltk.sem.Expression.fromstring('bullshit(leader)')\n",
    "bx = nltk.sem.Expression.fromstring(r'\\x.bullshit(x)')\n",
    "bx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "<e,t>\n"
     ]
    }
   ],
   "source": [
    "tlp = nltk.sem.logic.LogicParser(True)\n",
    "print(tlp.parse('man(x)').type)\n",
    "\n",
    "sig = {'man': '<e, t>'}\n",
    "e = tlp.parse('man(x)', sig)\n",
    "print(e.function.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model semantics\n",
    "\n",
    "Model = Domain + Valution + Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'l'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val=Valuation([('leader', 'l'),('bullshit', {'l'})])\n",
    "dom = val.domain\n",
    "m = Model(dom, val)\n",
    "m.evaluate('bullshit(leader)', {})\n",
    "\n",
    "dom = val.domain\n",
    "g = nltk.sem.Assignment(dom, [('x', 'l')])\n",
    "\n",
    "m.evaluate('bullshit(x)', g)\n",
    "\n",
    "g = nltk.sem.Assignment(dom, [])\n",
    "b = Expression.fromstring('bullshit(x)')\n",
    "m.satisfiers(b, 'x', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model satifying the formulas\n",
    "\n",
    "- Mace: find a model\n",
    "- MaceCommand: find a conterexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Expression.fromstring('exists x.(man(x) & walks(x))')\n",
    "c1 = Expression.fromstring('mortal(socrates)')\n",
    "c2 = Expression.fromstring('all x. (man(x) -> mortal(x))')\n",
    "c3 = Expression.fromstring('man(socrates)')\n",
    "c4 = Expression.fromstring('-mortal(socrates)')\n",
    "mb = nltk.Mace(5)\n",
    "mb.build_model(None, [c1, c2])\n",
    "mb.build_model(None, [c3, c2, c4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Expression.fromstring('all x. (man(x) -> love(x, lucy) | love(x, mary))')\n",
    "b = Expression.fromstring('man(bob)')\n",
    "c = Expression.fromstring('man(carl)')\n",
    "d = Expression.fromstring('love(carl, mary)')\n",
    "mc = nltk.MaceCommand(d, assumptions=[a, b, c])\n",
    "mc.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 'a',\n",
      " 'carl': 'a',\n",
      " 'love': {('a', 'a')},\n",
      " 'lucy': 'a',\n",
      " 'man': {('a',)},\n",
      " 'mary': 'b'}\n"
     ]
    }
   ],
   "source": [
    "print(mc.valuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lambda calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\\X x.X(\\y.chase(x,y)))(\\P.exists x.(dog(x) & P(x)))\n",
      "\\x.exists z11.(dog(z11) & chase(x,z11))\n",
      "\\Z x.Z(\\y.chase(x,y))\n",
      "(\\X x.X(\\y.chase(x,y)))(\\P.exists x.(dog(x) & P(x)))\n",
      "\\x.exists z12.(dog(z12) & chase(x,z12))\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tvp=Expression.fromstring('\\X x. X(\\y.chase(x, y))')\n",
    "np=Expression.fromstring('\\P.exists x.(dog(x) & P(x))')\n",
    "vp=nltk.ApplicationExpression(tvp, np)\n",
    "print(vp)\n",
    "print(vp.simplify())  # beta-convert\n",
    "print(tvp.alpha_convert(nltk.Variable('Z')))\n",
    "\n",
    "vp1=Expression.fromstring('(\\X x. X(\\y.chase(x, y)))(\\P.exists x.(dog(x) & P(x)))')\n",
    "print(vp1)\n",
    "print(vp1.simplify())  # beta-convert\n",
    "print(vp1==vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ *type* = 'S'                                    ]\n",
      "[ SEM    = <all z20.(boy(z20) -> see(cyril,z20))> ]\n"
     ]
    }
   ],
   "source": [
    "grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
    "parser = nltk.load_parser(grammar_file, trace=0)\n",
    "s = 'Cyril sees every boy'\n",
    "t = s.split()\n",
    "trees = parser.parse(t)\n",
    "for tree in trees:\n",
    "    print(tree.label())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[SEM=<all z22.(boy(z22) -> see(cyril,z22))>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(cyril)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(cyril)>] Cyril))\n",
      "  (VP[NUM='sg', SEM=<\\x.all z22.(boy(z22) -> see(x,z22))>]\n",
      "    (TV[NUM='sg', SEM=<\\X x.X(\\y.see(x,y))>, TNS='pres'] sees)\n",
      "    (NP[NUM='sg', SEM=<\\Q.all x.(boy(x) -> Q(x))>]\n",
      "      (Det[NUM='sg', SEM=<\\P Q.all x.(P(x) -> Q(x))>] every)\n",
      "      (Nom[NUM='sg', SEM=<\\x.boy(x)>]\n",
      "        (N[NUM='sg', SEM=<\\x.boy(x)>] boy)))))\n",
      "all z22.(boy(z22) -> see(cyril,z22)) False\n"
     ]
    }
   ],
   "source": [
    "val=Valuation([('cyril', 'c'),('bertie', 'b'), ('boy', {'b'}), ('see', {('m', 'b')})])\n",
    "dom = val.domain\n",
    "m = Model(dom, val)\n",
    "g = nltk.sem.Assignment(dom)\n",
    "res = nltk.sem.evaluate_sents([s], grammar_file, m, g)[0]\n",
    "for syn, sem, val in res:\n",
    "    print(syn)\n",
    "    print(sem, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooper Storage\n",
    "\n",
    "- bind operator\n",
    "- S-index\n",
    "\n",
    "```\n",
    "Every girl chases a dog.\n",
    "core: chase(x,y)\n",
    "bind: \\P. exists y. (dog(y) & P(y)) (\\z2. chase(z1, z2))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CORE=<chase(z2,z3)>, STORE=(bo(\\P.all x.(girl(x) -> P(x)),z2), bo(\\P.exists x.(dog(x) & P(x)),z3))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chase(z2,z3)\n"
     ]
    }
   ],
   "source": [
    "from nltk.sem import cooper_storage\n",
    "s = 'every girl chases a dog'\n",
    "t = cooper_storage.parse_with_bindops(s, grammar='grammars/book_grammars/storage.fcfg')\n",
    "semrep = t[0].label()['SEM']\n",
    "semrep\n",
    "cs_semrep = cs.CooperStore(semrep)\n",
    "print(cs_semrep.core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bo(\\P.all x.(girl(x) -> P(x)),z2)\n",
      "bo(\\P.exists x.(dog(x) & P(x)),z3)\n"
     ]
    }
   ],
   "source": [
    "for bo in cs_semrep.store:\n",
    "    print(bo)  # bind operator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
